Job:
    run_mode: "Training"  
    #{Training, Predict, Repeat, CV, Hyperparameter, Ensemble, Analysis}
    Training:
        job_name: "my_train_job"
        reprocess: "False"    
        model: MultiScaleGNN_demo #CGCNN_demo MultiScaleGNN_demo MPNN_demo MEGNet_demo
        load_model: "False"
        save_model: "True"
        model_path: "MultiScaleGNN_model.pth" #MultiScaleGNN_model.pth CGCNN_model.pth MPNN_model.pth
        write_output: "True"
        parallel: "False"
        #seed=0 means random initalization
        seed: 0        
    Predict:
        job_name: "my_predict_job"
        reprocess: "False"    
        model_path: "MultiScaleGNN_model.pth"
        write_output: "True"
        seed: 0     
    Repeat:
        job_name: "my_repeat_job"
        reprocess: "False"    
        model: MultiScaleGNN_demo   
        model_path: "my_model.pth"
        write_output: "False"
        parallel: "True"
        seed: 0      
        ###specific options
        #number of repeat trials
        repeat_trials: 5
    CV:
        job_name: "my_CV_job"
        reprocess: "False"    
        model: CGCNN_demo   
        write_output: "True"
        parallel: "True"
        seed: 0     
        ###specific options
        #number of folds for n-fold CV
        cv_folds: 5    
    Hyperparameter:
        job_name: "my_hyperparameter_job"
        reprocess: "False"    
        model: CGCNN_demo   
        seed: 0  
        ###specific options   
        hyper_trials: 10
        #number of concurrent trials (can be greater than number of GPUs)
        hyper_concurrency: 8
        #frequency of checkpointing and update (default: 1)
        hyper_iter: 1
        #resume a previous hyperparameter optimization run
        hyper_resume: "True"
        #Verbosity of ray tune output; available: (1, 2, 3)
        hyper_verbosity: 1
        #Delete processed datasets
        hyper_delete_processed: "True"
    Ensemble:
        job_name: "my_ensemble_job"
        reprocess: "False"     
        save_model: "False"
        model_path: "my_model.pth"
        write_output: "Partial"
        parallel: "True"
        seed: 0   
        ###specific options  
        #List of models to use: (Example: "CGCNN_demo,MPNN_demo,SchNet_demo,MEGNet_demo" or "CGCNN_demo,CGCNN_demo,CGCNN_demo,CGCNN_demo")
        ensemble_list: "CGCNN_demo,CGCNN_demo,CGCNN_demo,CGCNN_demo,CGCNN_demo"
    Analysis:
        job_name: "my_job" 
        reprocess: "False"   
        model: MultiScaleGNN_demo   
        model_path: "MultiScaleGNN_model.pth"
        write_output: "True"
        seed: 0     
    
Processing:
    #Whether to use "inmemory" or "large" format for pytorch-geometric dataset. Reccomend inmemory unless the dataset is too large
    dataset_type: "large"  
    adsorption: "True"
    #Path to data files
    data_path: "data/MOF_data/MOF_data" 
    #Path to target file within data_path
    target_path: "all_MOFs_screening_data_ads.csv" #all_MOFs_screening_data_ads.csv CoREMOF_CO2N2.csv
    #Method of obtaining atom idctionary: available:(provided, default, blank, generated)
    dictionary_source: "default"   
    #Path to atom dictionary file within data_path
    dictionary_path: "atom_dict.json"     
    #Format of data files (limit to those supported by ASE)
    data_format: "cif"
    #Print out processing info 
    verbose: "True"
    use_lattice: "True"
    if_adsorption: "True"
    #graph specific settings 
    graph_max_radius : 15.0
    graph_max_neighbors : 12
    voronoi: "False"
    edge_features: "True"
    graph_edge_length : 50 
    #SM specific settings
    SM_descriptor: "False"
    #SOAP specific settings
    SOAP_descriptor: "False"
    SOAP_rcut : 8.0
    SOAP_nmax : 6
    SOAP_lmax : 4
    SOAP_sigma : 0.3

Training:     
    #Index of target column in targets.csv
    target_index: 3 #1 for selectivity;3 for adsorption;4 for PLD; 5 for LCD
    #Loss functions (from pytorch) examples: l1_loss, mse_loss, binary_cross_entropy
    loss: "l1_loss"       
    #Ratios for train/val/test split out of a total of 1  
    train_ratio: 0.8
    val_ratio: 0.05
    test_ratio: 0.15
    #Training print out frequency (print per n number of epochs)
    verbosity: 1
    
Models:        
    CGCNN_demo:
        model: CGCNN
        dim1: 200 #175 #150
        dim2: 256 #256 #200
        pre_fc_count: 3 #1
        gc_count: 3 #2
        post_fc_count: 2 #1
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 100
        filename: CGCNN_temp.pth
        lr: 0.003 #0.001#0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.9, "patience":20, "min_lr":0.00001, "threshold":0.0002} 
    MultiScaleGNN_demo:
        model: MultiScaleConvNet
        dim1: 100 #100 95
        dim2: 128 #128 100
        n_output: 1
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 150 #250
        filename: MultiScaleGNN_temp.pth
        lr: 0.0015 #0.0012
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.7, "patience":8, "min_lr":0.00001, "threshold":0.0002}

    MPNN_demo:
        model: MPNN
        dim1: 50 #100
        dim2: 50 #100
        dim3: 50 #100
        pre_fc_count: 1
        gc_count: 2
        post_fc_count: 2
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 100
        filename: MPNN_temp.pth
        lr: 0.002 #0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}               
    SchNet_demo:
        model: SchNet
        dim1: 100
        dim2: 100
        dim3: 150
        cutoff: 8
        pre_fc_count: 1
        gc_count: 4
        post_fc_count: 3
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 100
        filename: SchNet_temp.pth
        lr: 0.0005
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}        
    MEGNet_demo:
        model: MEGNet
        dim1: 128
        dim2: 128
        dim3: 128
        pre_fc_count: 1
        gc_count: 2
        gc_fc_count: 3
        post_fc_count: 2
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 100
        filename: MEGNet_temp.pth
        lr: 0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":20, "min_lr":0.00001, "threshold":0.0002}              
    GCN_demo:
        model: GCN
        dim1: 100 
        dim2: 150
        pre_fc_count: 1
        gc_count: 4
        post_fc_count: 3
        pool: "global_mean_pool"
        pool_order: "early"
        batch_norm: "True"
        batch_track_stats: "True"
        act: "relu"
        dropout_rate: 0.0
        epochs: 100
        filename: GCN_temp.pth
        lr: 0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}     
    SM_demo:
        model: SM
        dim1: 100
        fc_count: 2
        epochs: 200
        lr: 0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}               
    SOAP_demo:
        model: SOAP
        dim1: 100
        fc_count: 2
        epochs: 200
        lr: 0.002
        batch_size: 100
        optimizer: "AdamW"
        optimizer_args: {}
        scheduler: "ReduceLROnPlateau"
        scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}

